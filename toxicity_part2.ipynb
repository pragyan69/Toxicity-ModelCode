{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = 'jigsaw-toxic-comment-classification-challenge/train.csv/train.csv'\n",
    "test_data_path = 'jigsaw-toxic-comment-classification-challenge/test.csv/test.csv'\n",
    "test_labels_path = 'jigsaw-toxic-comment-classification-challenge/test_labels.csv/test_labels.csv'\n",
    "\n",
    "train_df = pd.read_csv(train_data_path)\n",
    "test_df = pd.read_csv(test_data_path)\n",
    "test_labels_df = pd.read_csv(test_labels_path)\n",
    "\n",
    "# Check that the test data and labels align (same order)\n",
    "test_df = test_df.merge(test_labels_df, on='id')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_labels = test_labels_df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FEATURES = 2000  # Adjust if needed\n",
    "OUTPUT_SEQUENCE_LENGTH = 1800  # Adjust if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('./boss.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TextVectorization(max_tokens=MAX_FEATURES,\n",
    "                               output_sequence_length=OUTPUT_SEQUENCE_LENGTH,\n",
    "                               output_mode='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.adapt(train_df['comment_text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(text):\n",
    "    # Vectorize input text\n",
    "    vectorized_text = vectorizer([text])  # Vectorize input text\n",
    "\n",
    "    # Correctly format the input for prediction (remove unnecessary np.expand_dims)\n",
    "    # Ensure that the vectorized_text is in the format the model expects: (batch_size, sequence_length)\n",
    "    prediction = model.predict(vectorized_text)  # Remove the np.expand_dims\n",
    "\n",
    "    # Convert probabilities to binary outputs\n",
    "    predicted_classes = (prediction > 0.5).astype(int)\n",
    "    return predicted_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 500ms/step\n",
      "Prediction for 'This is a great example to test!': [[0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"This is a great example to test!\"\n",
    "prediction = make_prediction(sample_text)\n",
    "print(f\"Prediction for '{sample_text}': {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4787/4787 [==============================] - 396s 83ms/step\n",
      "Predictions on test data completed.\n"
     ]
    }
   ],
   "source": [
    "def predict_test_data(test_df):\n",
    "    # Vectorize the test data comments\n",
    "    test_vectorized = vectorizer(test_df['comment_text'].values)\n",
    "\n",
    "    # Create a Dataset from the vectorized data\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices(test_vectorized)\n",
    "    \n",
    "    # Batch the dataset\n",
    "    test_dataset = test_dataset.batch(32)\n",
    "\n",
    "    # Predict in batches (better for large datasets)\n",
    "    predictions = model.predict(test_dataset)\n",
    "\n",
    "    # Convert probabilities to binary outputs\n",
    "    predicted_classes = (predictions > 0.5).astype(int)\n",
    "    return predicted_classes\n",
    "\n",
    "# Get predictions for all test data\n",
    "test_predictions = predict_test_data(test_df)\n",
    "print(\"Predictions on test data completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.89\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Convert actual labels and predictions to tensors for computation\n",
    "actual_labels_tensor = tf.convert_to_tensor(actual_labels, dtype=tf.int32)\n",
    "predictions_tensor = tf.convert_to_tensor(test_predictions, dtype=tf.int32)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = tf.metrics.Accuracy()\n",
    "accuracy.update_state(actual_labels_tensor, predictions_tensor)\n",
    "overall_accuracy = accuracy.result().numpy()\n",
    "print(f\"Overall Accuracy: {overall_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 57ms/step\n",
      "The comment is: Not Toxic\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "The comment is: Toxic\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "The comment is: Not Toxic\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "The comment is: Not Toxic\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "The comment is: Not Toxic\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Interactive input for user to test comments\n",
    "while True:\n",
    "    input_text = input(\"Enter a comment to test for toxicity (type 'exit' to quit): \")\n",
    "    if input_text.lower() == 'exit':\n",
    "        break\n",
    "    result = make_prediction(input_text)\n",
    "    print(f\"The comment is: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
